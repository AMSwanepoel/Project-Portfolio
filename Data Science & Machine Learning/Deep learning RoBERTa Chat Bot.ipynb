{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "static-julian",
   "metadata": {},
   "source": [
    "# Deep learning Q&A Artificial Intelligence Chat Bot; A Transformer-Based Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pharmaceutical-hanging",
   "metadata": {},
   "source": [
    "#### Alexander Swanepoel, _University of Warwick, Computer Science MSc_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "criminal-piece",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "partial-yacht",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "married-thanks",
   "metadata": {},
   "source": [
    "We seek to explore the Deep learning approach to artificial intelligence chat bot construction. The application of a transformer based model has had great success with respect to natural language processing relative to state-of-the-art machine learning advances such as the OpenAI-GPT. The Model in particular we are interested in is a refined, robust version of the novel BERT ( Bidirectional Encoder Representations from Transformers) model, created and constructed by Lee at al. (https://arxiv.org/abs/1810.04805), known as RoBERTa. The model itself is keystoned on a pretraining ensemble, that allows for dynamic mask changing, and more fine-tuned hyperparameterisation. The results of the canonical predecessor can be seen tabulated below, however note that RoBERTa is noted in the white-paper to outperform BERT on every metric thereafter.\n",
    "\n",
    "System                  | MultiNLI | Question NLI | SWAG\n",
    "----------------------- | :------: | :----------: | :------:\n",
    "BERT                    | **86.7** | **91.1**     | **86.3**\n",
    "OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0\n",
    "\n",
    "The following notebook undergoes a demonstratory journey, to the application and exhibition of the results of the RoBERTa algorithm, such that one can experience and test within their own satisfactions. Thus, for completion the will explore the mathematical frameworks of the transformer deep learning model for completeness, and give a brief insight as to the operation at hand. Then, we will build an application such that a rudimentary version of the model can be tested and sandboxed. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "introductory-bullet",
   "metadata": {},
   "source": [
    "## Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "handmade-yukon",
   "metadata": {},
   "source": [
    "We seek to give brief insight into the workings of a transformer model initially developed by Vaswani et. al in the paper 'Attention is All You Need' (reference: https://arxiv.org/abs/1706.03762). The context of the model for succinctness will be in a natural language processing setting. Given a black-box setting, the given input would undergo some transformation, and yield a particular output, with some stochastic accounting. For example, if the given input into the transformer black box would be the string 'Je suis etudiant', an ouput would read 'I am a student', where the translation itself would be a set of instructions such that the encoding of the initial input could be decoded via a subset of connections, into a tangible string. With respect to the initial paper, on a high level viewpoint, they presume to stack 6 encoders ontop of another, with connections between the encoders and an identical number of decoders.\n",
    "\n",
    "The encoders themselves are comprised of a feed-forward neural network, essentially the canonical multi-layer perceptron(https://en.wikipedia.org/wiki/Feedforward_neural_network), and a self attention layer. The area of technical innovation would be the self attention layer. The self-attention layer can be locally described as a association mechanism for the algorithm. For example, if you were to pose the question, 'The chicken did not cross the road, as it was exhausted.' from an algorithmic perspective you would have to ponder the word 'it'. What is it in reference to? Is it the road, or the chicken? Its a triviality to humans, however to the machine learning model it is quite a complex process. In essence, the self-attention layer from ones understanding, evaluates the weightings and positions of the individual words in the input sequence, i.e. as the words are processed (with respect to each position in the input sequence), the layer searches for information gathered to optimise encoding for a particular word. \n",
    "\n",
    "This complicated mechanism is undergone in a similar fashion to recurrent neural network (RNN) machinery. When we consider how in RNN, the network itself maintains a hidden intermediate state that allows the respresentation of previous vectors(words), it has processed with respect to the current vector, the self attention layer behaves in a similar way. The self attention layer itself is where the transformer forms an 'understanding' of the relevancy of vectors with respect to the current vector being processed. For further understanding, please see https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "developing-infrared",
   "metadata": {},
   "source": [
    "### Mathematical Framework "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "positive-offset",
   "metadata": {},
   "source": [
    "For brevity, we will briefly discuss the computation of the self-attention mathematics for a deeper understanding. \n",
    "\n",
    "As discussed previously, self-attention layers can be represented as a scaled dot-product attention units. Passing a string into the transformer, will cause attention weightings to be computed simulatenously. The unit itself fabricates embeddings for every tokenised identifier i.e. word, referenced as token in context, that can contain information about the word itself, concurrently with a weighted combination of other related words, each with their respective weightings assigned by the attention weight.\n",
    "\n",
    "Then, for each unit, the transformer requires three learning matrices; the query weightings, $W_Q$, the key weightings, $W_K$, and the value weightings, $W_V$. For a given token, i, the word embedding $x_i$, is factored by all the matrices, such that the output is a given query vector, $q_i = x_i W_Q$, key vector, $k_i = x_i W_V$, and a value vector, $v_i = x_i W_V$. Individual attention weights, $a_{ij}$ are computed via the dot product of the i'th and the j'th component of $q_i \\bullet k_j$. Then, one can take the root with respec to the dimension of the key vector, to account for gradient stabilisation during training, and normalise the calculations via the softmax function. Retrospectively, $W_Q$ and $W_K$ are non-symmetric matrices, i.e. there is no transitive relationship between the ij'th component and the ji'th component. \n",
    "\n",
    "Then, the attention, can be defined as the following;\n",
    "\\begin{equation*}\n",
    "Attention(Q,K,V) = Softmax \\left((\\frac{QK^T}{\\sqrt{d_k}}\\right)V\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "signal-newport",
   "metadata": {},
   "source": [
    "## Construction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stuck-neutral",
   "metadata": {},
   "source": [
    "#### Prerequisite Installations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "random-creation",
   "metadata": {},
   "source": [
    "Below, we will install the roberta-base for QA model (see https://huggingface.co/deepset/roberta-base-squad2 for more information), and load the required dependencies and model required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "visible-tomorrow",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\alexs\\anaconda3\\envs\\rpythonenv\\lib\\site-packages (4.16.2)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,>=0.10.1 in c:\\users\\alexs\\anaconda3\\envs\\rpythonenv\\lib\\site-packages (from transformers) (0.11.6)\n",
      "Requirement already satisfied: requests in c:\\users\\alexs\\anaconda3\\envs\\rpythonenv\\lib\\site-packages (from transformers) (2.25.1)\n",
      "Requirement already satisfied: sacremoses in c:\\users\\alexs\\anaconda3\\envs\\rpythonenv\\lib\\site-packages (from transformers) (0.0.47)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\alexs\\anaconda3\\envs\\rpythonenv\\lib\\site-packages (from transformers) (2021.3.17)\n",
      "Requirement already satisfied: filelock in c:\\users\\alexs\\anaconda3\\envs\\rpythonenv\\lib\\site-packages (from transformers) (3.6.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in c:\\users\\alexs\\anaconda3\\envs\\rpythonenv\\lib\\site-packages (from transformers) (0.4.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\alexs\\anaconda3\\envs\\rpythonenv\\lib\\site-packages (from transformers) (1.19.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\alexs\\anaconda3\\envs\\rpythonenv\\lib\\site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\alexs\\anaconda3\\envs\\rpythonenv\\lib\\site-packages (from transformers) (5.4.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\alexs\\anaconda3\\envs\\rpythonenv\\lib\\site-packages (from transformers) (4.63.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\alexs\\anaconda3\\envs\\rpythonenv\\lib\\site-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.7.4.3)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\alexs\\anaconda3\\envs\\rpythonenv\\lib\\site-packages (from packaging>=20.0->transformers) (2.4.7)\n",
      "Requirement already satisfied: colorama in c:\\users\\alexs\\anaconda3\\envs\\rpythonenv\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\alexs\\anaconda3\\envs\\rpythonenv\\lib\\site-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\alexs\\anaconda3\\envs\\rpythonenv\\lib\\site-packages (from requests->transformers) (2021.10.8)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in c:\\users\\alexs\\anaconda3\\envs\\rpythonenv\\lib\\site-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\alexs\\anaconda3\\envs\\rpythonenv\\lib\\site-packages (from requests->transformers) (1.26.4)\n",
      "Requirement already satisfied: six in c:\\users\\alexs\\anaconda3\\envs\\rpythonenv\\lib\\site-packages (from sacremoses->transformers) (1.15.0)\n",
      "Requirement already satisfied: joblib in c:\\users\\alexs\\anaconda3\\envs\\rpythonenv\\lib\\site-packages (from sacremoses->transformers) (1.0.1)\n",
      "Requirement already satisfied: click in c:\\users\\alexs\\anaconda3\\envs\\rpythonenv\\lib\\site-packages (from sacremoses->transformers) (7.1.2)\n"
     ]
    }
   ],
   "source": [
    "#Transformer installation see https://huggingface.co/deepset/roberta-base-squad2\n",
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "smoking-blocking",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dependency Import with respect to RoBERTa Model\n",
    "from transformers import AutoModelForQuestionAnswering\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "variable-intro",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Instantiating the Pipeline for Questions and Answers, and Model Load\n",
    "model_name = \"deepset/roberta-base-squad2\"\n",
    "\n",
    "nlp = pipeline(model=model_name, tokenizer=model_name, revision=\"v1.0\", task=\"question-answering\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "inappropriate-transaction",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Corpus, body of text (reference: https://en.wikipedia.org/wiki/Spider-Man)\n",
    "wikipedia_text = \"\"\"\n",
    "Spider-Man is a superhero appearing in American comic books published by Marvel Comics. Created by writer-editor Stan Lee and artist Steve Ditko, he first appeared in the anthology comic book Amazing Fantasy #15 (August 1962) in the Silver Age of Comic Books. He has since been featured in movies, television shows, video games, and plays. Spider-Man is the alias of Peter Parker, an orphan raised by his Aunt May and Uncle Ben in New York City after his parents Richard and Mary Parker died in a plane crash. Lee and Ditko had the character deal with the struggles of adolescence and financial issues and gave him many supporting characters, such as Flash Thompson, J. Jonah Jameson and Harry Osborn, romantic interests Gwen Stacy, Mary Jane Watson and the Black Cat, and foes such as Doctor Octopus, the Green Goblin and Venom. In his origin story, he gets spider-related abilities from a bite from a radioactive spider; these include clinging to surfaces, superhuman strength and agility, and detecting danger with his \"spider-sense.\" He also builds wrist-mounted \"web-shooter\" devices that shoot artificial spider webs of his own design.\n",
    "\n",
    "When Spider-Man first appeared in the early 1960s, teenagers in superhero comic books were usually relegated to the role of sidekick to the protagonist. The Spider-Man series broke ground by featuring Peter Parker, a high school student from Queens behind Spider-Man's secret identity and with whose \"self-obsessions with rejection, inadequacy, and loneliness\" young readers could relate.[9] While Spider-Man had all the makings of a sidekick, unlike previous teen heroes such as Bucky and Robin, Spider-Man had no superhero mentor like Captain America and Batman; he thus had to learn for himself that \"with great power there must also come great responsibility\"—a line included in a text box in the final panel of the first Spider-Man story but later retroactively attributed to his guardian, his late Uncle Ben Parker.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "leading-sunglasses",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dictionary Pass Through; requires a Question and Context\n",
    "# Define question set\n",
    "question_set = {\n",
    "                'question':'Who created Spiderman?', \n",
    "                'context':wikipedia_text\n",
    "               }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ordered-chocolate",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alexs\\anaconda3\\envs\\RPythonEnv\\lib\\site-packages\\numpy\\core\\_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return array(a, dtype, copy=False, order=order)\n"
     ]
    }
   ],
   "source": [
    "results = nlp(question_set, dtype=object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "needed-circuit",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Stan Lee and artist Steve Ditko'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results['answer']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "operational-pacific",
   "metadata": {},
   "source": [
    "#### Anvil Install & Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "healthy-holder",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: anvil-uplink in c:\\users\\alexs\\anaconda3\\envs\\rpythonenv\\lib\\site-packages (0.3.42)\n",
      "Requirement already satisfied: six in c:\\users\\alexs\\anaconda3\\envs\\rpythonenv\\lib\\site-packages (from anvil-uplink) (1.15.0)\n",
      "Requirement already satisfied: future in c:\\users\\alexs\\anaconda3\\envs\\rpythonenv\\lib\\site-packages (from anvil-uplink) (0.18.2)\n",
      "Requirement already satisfied: ws4py in c:\\users\\alexs\\anaconda3\\envs\\rpythonenv\\lib\\site-packages (from anvil-uplink) (0.5.1)\n",
      "Collecting argparse\n",
      "  Using cached argparse-1.4.0-py2.py3-none-any.whl (23 kB)\n",
      "Installing collected packages: argparse\n",
      "Successfully installed argparse-1.4.0\n"
     ]
    }
   ],
   "source": [
    "!pip install anvil-uplink"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ideal-breed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import anvil.server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fiscal-prime",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting to wss://anvil.works/uplink\n",
      "Anvil websocket open\n",
      "Connected to \"Default environment\" as SERVER\n"
     ]
    }
   ],
   "source": [
    "anvil.server.connect('HMKZ6CRFQU23PH7ZEMHND45W-GYOCEGB4RZII3GIA')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "worse-client",
   "metadata": {},
   "source": [
    "##### Callable Function Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "relative-midwest",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tells the jupyter server that this is a an Anvil callable function\n",
    "@anvil.server.callable\n",
    "# Define the function that is going to do our NLP\n",
    "def answer_questions(question_text, context_text): \n",
    "    # Convert this to a dictionary\n",
    "    question_set = {\n",
    "                'question':question_text, \n",
    "                'context':context_text\n",
    "               }\n",
    "    # Run it through the NLP pipeline\n",
    "    results = nlp(question_set)\n",
    "    \n",
    "    return results['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "unique-happening",
   "metadata": {},
   "outputs": [],
   "source": [
    "anvil_result = answer_questions('Who is Spiderman\\'s enemy?', wikipedia_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "separate-computer",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Doctor Octopus'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anvil_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caroline-consideration",
   "metadata": {},
   "source": [
    "## Conclusion "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "packed-cement",
   "metadata": {},
   "source": [
    "The results themselves are remarkable, the notebook itself has provided a succinct introductory version of transformer deep learning, with library of canonical references to state-of-the-art machinery. The hope is we may find use for the pedagogical approach provided at a future date, to expand on the research provided and build more complex systems. \n",
    "\n",
    "With respect to the app constructed, we provide a private link to the app sandbox where one can explore the use of the app at their pleasure. The application itself is constructed as a parsing tool, designed so that one may pass in a body of text, from any source, in which one may pose a question with reference to the original text, and the AI will respond with a deep-learning constructed answer. The link for the application is as follows: https://gyocegb4rzii3gia.anvil.app/TJLRBEUDIC2J6NUXQHARB6FI (Please share with discretion as server space and bandwidth is limited). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intense-roulette",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
